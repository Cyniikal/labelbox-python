{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mounted-asian",
   "metadata": {
    "id": "EyNkbpW7ouEf"
   },
   "source": [
    "  \n",
    "<td>\n",
    "    <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>\n",
    "</td>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-lemon",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Model Diagnostics - Custom Metrics\n",
    "\n",
    "\n",
    "* The main idea behind custom metrics is that by enabling users to provide specific metrics, that closely align with company objectives, users will be able to iterate faster and more track model quality over time with less noise.\n",
    "\n",
    "* For example, a self driving car dashcam might track the precision, recall, and iou of their model. However, nearby objects, such as people, might matter much more than far away objects. This important aspect of model quality is lost to the noise of these broad metrics. \n",
    "* Continuing with this example, the organization might want to report safety in terms of performance of the model on nearby objects. They maybe also want to know how much marginal value they are getting out of each training example. Both of these use cases rely on the metric specifically tracking the company's target objective.\n",
    "\n",
    "\n",
    "<b>Topics Covered</b>\n",
    "* Custom metrics basics\n",
    "* Complete diagnostics demo using custom metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-magic",
   "metadata": {
    "id": "subsequent-magic"
   },
   "source": [
    "## Environment Setup\n",
    "\n",
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "voluntary-minister",
   "metadata": {
    "id": "voluntary-minister"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \"labelbox[data]\" \\\n",
    "             scikit-image \\\n",
    "             tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wooden-worship",
   "metadata": {
    "id": "wooden-worship"
   },
   "outputs": [],
   "source": [
    "# Run these if running in a colab notebook\n",
    "COLAB = \"google.colab\" in str(get_ipython())\n",
    "\n",
    "if COLAB:\n",
    "    !git clone https://github.com/Labelbox/labelbox-python.git\n",
    "    !cd labelbox-python\n",
    "    !mv labelbox-python/examples/model_assisted_labeling/*.py ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-leone",
   "metadata": {
    "id": "latter-leone"
   },
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "committed-richards",
   "metadata": {
    "id": "committed-richards"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../model_assisted_labeling')\n",
    "\n",
    "import uuid\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "import requests\n",
    "from tqdm import notebook\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from labelbox.schema.ontology import OntologyBuilder, Tool\n",
    "from labelbox.data.metrics.processing import get_label_pairs\n",
    "from labelbox import Client, LabelingFrontend, MALPredictionImport\n",
    "from labelbox.data.metrics.iou import data_row_miou, feature_miou_metric\n",
    "from labelbox.data.serialization import NDJsonConverter\n",
    "from labelbox.data.annotation_types import (\n",
    "    ScalarMetric, \n",
    "    LabelList, \n",
    "    Label, \n",
    "    ImageData, \n",
    "    MaskData,\n",
    "    Mask, \n",
    "    Polygon,\n",
    "    Point, \n",
    "    Rectangle, \n",
    "    ObjectAnnotation,\n",
    "    ClassificationAnnotation,\n",
    "    ClassificationAnswer,\n",
    "    Radio\n",
    ")\n",
    "\n",
    "try:\n",
    "    from image_model import predict, load_model\n",
    "except ModuleNotFoundError: \n",
    "    # !git clone https://github.com/Labelbox/labelbox-python.git\n",
    "    # !cd labelbox-python && git checkout mea-dev\n",
    "    # !mv labelbox-python/examples/model_assisted_labeling/*.py .\n",
    "    raise Exception(\"You will need to run from the labelbox-python git repo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-tribe",
   "metadata": {},
   "source": [
    "## Custom Metrics\n",
    "* Users can provide metrics to provide metric information at different levels of granularity.\n",
    "    * Users can provide metrics for \n",
    "        1. data rows\n",
    "        2. features\n",
    "        3. subclasses\n",
    "    * Additionally, metrics can be given custom names to best describe what they are measuring.\n",
    "    \n",
    "* Limits and Behavior:\n",
    "    * At a data row cannot have more than 20 metrics\n",
    "    * Metrics are upserted, so if a metric already exists, its value will be replaced\n",
    "    * Metrics can have values in the range [0,100000]\n",
    "* Currently `ScalarMetric`s and `ConfusionMatrixMetric`s are supported. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-affect",
   "metadata": {},
   "source": [
    "### ScalarMetric\n",
    "    - A `ScalarMetric` is a metric with just a single scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "palestinian-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from labelbox.data.annotation_types import (\n",
    "        ScalarMetric, \n",
    "    ScalarMetricAggregation, \n",
    "    ConfusionMatrixMetric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sudden-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row_metric = ScalarMetric(\n",
    "    metric_name = \"iou\",\n",
    "    value = 0.5\n",
    ")\n",
    "\n",
    "feature_metric = ScalarMetric(\n",
    "    metric_name = \"iou\",\n",
    "    feature_name = \"cat\",\n",
    "    value = 0.5\n",
    ")\n",
    "\n",
    "subclass_metric = ScalarMetric(\n",
    "    metric_name = \"iou\",\n",
    "    feature_name = \"cat\",\n",
    "    subclass_name = \"organge\",\n",
    "    value = 0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-patient",
   "metadata": {},
   "source": [
    "### ConfusionMatrixMetric\n",
    "- A `ConfusionMatrixMetric` contains 4 numbers [True postivie, False Postive, True Negative, False Negateive]\n",
    "- Confidence is also supported a key value pairs, where the score is the key and the value is the metric value.\n",
    "- In the user interface, these metrics are used to derive precision,recall, and f1 scores. The reason these are not directly uploaded is that the raw data allows us to do processing on the front end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "literary-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_row_metric = ConfusionMatrixMetric(\n",
    "    metric_name = \"confusion_matrix_50pct_iou\",\n",
    "    feature_name = \"cat\",  \n",
    "    subclass_name = \"organge\",    \n",
    "    value = [1,0,1,0]\n",
    ")\n",
    "\n",
    "\n",
    "feature_metric = ConfusionMatrixMetric(\n",
    "    metric_name = \"confusion_matrix_50pct_iou\",\n",
    "    feature_name = \"cat\",  \n",
    "    subclass_name = \"organge\",    \n",
    "    value = [1,0,1,0]\n",
    ")\n",
    "\n",
    "subclass_metric = ConfusionMatrixMetric(\n",
    "    metric_name = \"confusion_matrix_50pct_iou\",\n",
    "    feature_name = \"cat\",  \n",
    "    subclass_name = \"organge\",    \n",
    "    value = [1,0,1,0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-portrait",
   "metadata": {},
   "source": [
    "### Confidence\n",
    "* Users can provide confidence scores along with metrics\n",
    "* This enables them to explore their model performance without necessarily knowing the optimal thresholds for each class.\n",
    "* Users can filter on confidence and value in the UI to perform powerful queries.\n",
    "* The keys represent a confidence score (must be between 0 and 1) and the values represent either a scalar metric or for confusion matrix metrics [TP,FP,TN,FN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "pursuant-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "confusion_matrix_metric_with_confidence = ConfusionMatrixMetric(\n",
    "    metric_name = \"confusion_matrix_50pct_iou\",\n",
    "    feature_name = \"cat\",  \n",
    "    subclass_name = \"organge\",    \n",
    "    value = {0.1 : [1,0,1,0], 0.3 : [1,0,1,0], 0.5 : [1,0,1,0], 0.7 : [1,0,1,0], 0.9 : [1,0,1,0]}\n",
    ")\n",
    "\n",
    "scalar_metric_with_confidence = ScalarMetric(\n",
    "    metric_name = \"iou\",\n",
    "    value = {0.1 : 0.2, 0.3 : 0.25, 0.5 : 0.3, 0.7 : 0.4, 0.9: 0.3}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-gather",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "* This is an optional field on the `ScalarMetric` object (by default it uses Arithmetic Mean).\n",
    "* Aggregations occur in two cases:\n",
    "    1. When a user provides a feature or subclass level metric, Labelbox automatically aggregates all metrics with the same parent to create a value for that parent.\n",
    "        * E.g. A user provides cat and dog iou. The data row level metric for iou is the average of both of those.\n",
    "        * The exception to this is when the data row level iou is explicitly set, then the aggregation will not take effect (on a per data row basis). \n",
    "    2. When users create slices or want aggregate statistics on their models, the selected aggregation is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acquired-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If the following metrics are uploaded then\n",
    "in the web app, users will see:\n",
    "true positives dog = 4\n",
    "true positives cat = 3\n",
    "true positives = 7\n",
    "\"\"\"\n",
    "\n",
    "feature_metric = ScalarMetric(\n",
    "    metric_name = \"true_positives\",\n",
    "    feature_name = \"cat\",\n",
    "    value = 3,\n",
    "    aggregation = ScalarMetricAggregation.SUM\n",
    ")\n",
    "\n",
    "feature_metric = ScalarMetric(\n",
    "    metric_name = \"true_positives\",\n",
    "    feature_name = \"dog\",\n",
    "    value = 4,\n",
    "    aggregation = ScalarMetricAggregation.SUM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-surveillance",
   "metadata": {},
   "source": [
    "### Built-in Metrics:\n",
    "* The SDK Provides a set of default metrics that make metrics easy to use.\n",
    "1. `confusion_matrix_metric()`\n",
    "    * Computes a single confusion matrix metric for all the predictions and labels provided. \n",
    "2. `miou_metric()`\n",
    "    * Computes a single iou score for all predictions and labels provided    \n",
    "3. `feature_confusion_matrix_metric()`\n",
    "    * Computes the iou score for each of the classes found in the predictions and labels\n",
    "4. `feature_miou_metric()`\n",
    "     * Computes a confusion matrix metric for each of the classes found in the predictions and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "conceptual-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "from labelbox.data.metrics import feature_miou_metric, miou_metric, confusion_matrix_metric, feature_confusion_matrix_metric\n",
    "predictions = [\n",
    "    ObjectAnnotation(\n",
    "        name=\"cat\",\n",
    "        value=Rectangle(start=Point(x=0, y=0),\n",
    "                        end=Point(x=10, y=10))\n",
    "    )\n",
    "]\n",
    "        \n",
    "ground_truths = [\n",
    "    ObjectAnnotation(\n",
    "        name=\"cat\",\n",
    "        value=Rectangle(start=Point(x=0, y=0),\n",
    "                        end=Point(x=8, y=8)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "least-facility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ScalarMetric(value=0.64, feature_name='cat', subclass_name=None, extra={}, metric_name='iou', aggregation=<ScalarMetricAggregation.ARITHMETIC_MEAN: 'ARITHMETIC_MEAN'>)]\n",
      "[ScalarMetric(value=0.64, feature_name=None, subclass_name=None, extra={}, metric_name='iou', aggregation=<ScalarMetricAggregation.ARITHMETIC_MEAN: 'ARITHMETIC_MEAN'>)]\n",
      "[ConfusionMatrixMetric(value=(1, 0, 0, 0), feature_name=None, subclass_name=None, extra={}, metric_name='confusion_matrix_50pct_iou', aggregation=<ConfusionMatrixAggregation.CONFUSION_MATRIX: 'CONFUSION_MATRIX'>)]\n",
      "[ConfusionMatrixMetric(value=(1, 0, 0, 0), feature_name='cat', subclass_name=None, extra={}, metric_name='confusion_matrix_50pct_iou', aggregation=<ConfusionMatrixAggregation.CONFUSION_MATRIX: 'CONFUSION_MATRIX'>)]\n"
     ]
    }
   ],
   "source": [
    "print(feature_miou_metric(ground_truths, predictions))\n",
    "print(miou_metric(ground_truths, predictions))\n",
    "print(confusion_matrix_metric(ground_truths, predictions))\n",
    "print(feature_confusion_matrix_metric(ground_truths, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "tracked-officer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ConfusionMatrixMetric(value=(0, 1, 0, 1), feature_name='cat', subclass_name=None, extra={}, metric_name='confusion_matrix_90pct_iou', aggregation=<ConfusionMatrixAggregation.CONFUSION_MATRIX: 'CONFUSION_MATRIX'>)]\n"
     ]
    }
   ],
   "source": [
    "# Adjust iou for iou calcuations.\n",
    "# Set it higher than 0.64 and we get a false postive and a false negative for the other ground truth object.\n",
    "print(feature_confusion_matrix_metric(ground_truths, predictions, iou = 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "quarterly-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subclasses are included by default\n",
    "predictions = [\n",
    "    ObjectAnnotation(\n",
    "        name=\"cat\",\n",
    "        value=Rectangle(start=Point(x=0, y=0),\n",
    "                        end=Point(x=10, y=10)),\n",
    "    classifications = [\n",
    "        ClassificationAnnotation(\n",
    "        name=\"height\", value=Radio(answer=ClassificationAnswer(name=\"tall\")))\n",
    "    \n",
    "    ])\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    ObjectAnnotation(\n",
    "        name=\"cat\",\n",
    "        value=Rectangle(start=Point(x=0, y=0),\n",
    "                        end=Point(x=10, y=10)),\n",
    "    classifications = [\n",
    "        ClassificationAnnotation(\n",
    "        name=\"height\", value=Radio(answer=ClassificationAnswer(name=\"short\")))\n",
    "    \n",
    "    ])\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "substantial-blank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subclasses: (0, 1, 0, 1)\n",
      "Excluding Subclasses: (1, 0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(\"Subclasses:\", feature_confusion_matrix_metric(ground_truths, predictions)[0].value)\n",
    "print(\"Excluding Subclasses:\", feature_confusion_matrix_metric(ground_truths, predictions, include_subclasses = False)[0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-ambassador",
   "metadata": {},
   "source": [
    "## Complete Example\n",
    "* Custom metrics are uploaded exactly the same way that iou was previously uploaded.\n",
    "* A metric must be added to a `Label` to create an association between a data row and the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "economic-chase",
   "metadata": {
    "id": "economic-chase"
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"Diagnostics Demo Custom Metrics\"\n",
    "MODEL_NAME = \"MSCOCO-Mapillary-Custom-Metrics\"\n",
    "MODEL_VERSION = \"0.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "affecting-myanmar",
   "metadata": {
    "id": "affecting-myanmar"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../model_assisted_labeling/image_model.py:17: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from gs://cloud-tpu-checkpoints/mask-rcnn/1555659850/variables/variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-13 08:27:06.891557: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n"
     ]
    }
   ],
   "source": [
    "client = Client(api_key=API_KEY)\n",
    "load_model() # initialize Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "modern-program",
   "metadata": {
    "id": "modern-program"
   },
   "outputs": [],
   "source": [
    "# Configure for whatever combination of tools and class names that you would like.\n",
    "class_mappings = {\n",
    "    1: {\"name\": 'person', \"kind\": Tool.Type.POLYGON},\n",
    "    2: {\"name\": 'bicycle', \"kind\": Tool.Type.SEGMENTATION, 'color' : 64},\n",
    "    3: {\"name\": 'car', \"kind\": Tool.Type.BBOX},\n",
    "    4: {\"name\": 'motorcycle', \"kind\": Tool.Type.BBOX},\n",
    "    6: {\"name\": 'bus', \"kind\": Tool.Type.POLYGON},\n",
    "    7: {\"name\": 'train', \"kind\": Tool.Type.POLYGON},\n",
    "    8: {\"name\": 'truck', \"kind\": Tool.Type.POLYGON},\n",
    "    10: {\"name\": 'traffic light', \"kind\": Tool.Type.POINT},\n",
    "    11: {\"name\": 'fire hydrant', \"kind\": Tool.Type.BBOX},\n",
    "    13: {\"name\": 'stop sign', \"kind\": Tool.Type.SEGMENTATION, 'color' : 255},\n",
    "    14: {\"name\": 'parking meter', \"kind\": Tool.Type.POINT},\n",
    "    28: {\"name\": 'umbrella', \"kind\": Tool.Type.SEGMENTATION, 'color' : 128},    \n",
    "    31: {\"name\": 'handbag', \"kind\": Tool.Type.POINT},        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-burden",
   "metadata": {
    "id": "dated-burden"
   },
   "source": [
    "## Create Predictions\n",
    "* Loop over data_rows, make predictions, and create ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "blank-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- setup dataset ---\n",
    "# load mapillary sample\n",
    "sample_csv_url = \"https://raw.githubusercontent.com/Labelbox/labelbox-python/develop/examples/assets/mapillary_sample.csv\"\n",
    "with requests.get(sample_csv_url, stream=True) as r:\n",
    "    image_data = [row.split(',') for row in (line.decode('utf-8') for line in r.iter_lines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "asian-savings",
   "metadata": {
    "id": "asian-savings"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914d3a6ed2de448e8a60a337a9d6b62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = LabelList()\n",
    "for (image_url, external_id) in notebook.tqdm(image_data[:5]):\n",
    "    image = ImageData(url = image_url, external_id = external_id)\n",
    "    height, width = image.value.shape[:2]\n",
    "    prediction = predict(np.array([image.im_bytes]), min_score=0.5, height=height, width = width)\n",
    "    boxes, classes, seg_masks = prediction[\"boxes\"], prediction[\"class_indices\"], prediction[\"seg_masks\"]\n",
    "    annotations = []\n",
    "    for box, class_idx, seg in zip(boxes, classes, seg_masks):\n",
    "        if class_idx in class_mappings:\n",
    "            class_info = class_mappings.get(class_idx)\n",
    "            if class_info['kind'] == Tool.Type.POLYGON:\n",
    "                contours = measure.find_contours(seg, 0.5)\n",
    "                pts = contours[0].astype(np.int32)\n",
    "                value = Polygon(points = [Point(x = x, y = y) for x,y in np.roll(pts, 1, axis=-1)])\n",
    "            elif class_info['kind'] == Tool.Type.BBOX:\n",
    "                value = Rectangle(start = Point(x = box[1], y = box[0]), end = Point(x=box[3], y=box[2]))\n",
    "            elif class_info['kind'] == Tool.Type.POINT:\n",
    "                value = Point(x=(box[1] + box[3]) / 2., y = (box[0] + box[2]) / 2.)\n",
    "            elif class_info['kind'] == Tool.Type.SEGMENTATION:\n",
    "                value = Mask(mask = MaskData.from_2D_arr(seg * class_info['color']), color = (class_info['color'],)* 3)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported kind found. {class_info['kind']}\")\n",
    "            annotations.append(ObjectAnnotation(name = class_info['name'], value = value))\n",
    "    predictions.append(Label(data = image, annotations = annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-suicide",
   "metadata": {},
   "source": [
    "## Setup a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "received-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "for target in class_mappings.values():\n",
    "     tools.append(Tool(tool=target['kind'], name=target[\"name\"]))\n",
    "ontology_builder = OntologyBuilder(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-structure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "stopped-phrase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up: Diagnostics Demo Custom Metrics\n",
      "Dataset Created: cktimhcy6c8mv0yba7onpdna9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Setting up: {PROJECT_NAME}\")\n",
    "\n",
    "project = client.create_project(name=PROJECT_NAME)\n",
    "editor = next(client.get_labeling_frontends(where=LabelingFrontend.name == \"Editor\"))\n",
    "project.setup(editor, ontology_builder.asdict())\n",
    "\n",
    "dataset = client.create_dataset(name=\"Mapillary Diagnostics Demo\")\n",
    "print(f\"Dataset Created: {dataset.uid}\")\n",
    "project.datasets.connect(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-shelf",
   "metadata": {},
   "source": [
    "## Prepare for upload\n",
    "* Our local annotations need the following:\n",
    "    1. signed url for segmentation masks\n",
    "    2. data rows in labelbox\n",
    "    3. feature schema ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "unavailable-egyptian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 37052.16it/s]\n",
      "5it [00:00, 46707.17it/s]\n",
      "5it [00:00, 34267.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<labelbox.data.annotation_types.collection.LabelList at 0x1811a3880>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signer = lambda _bytes: client.upload_data(content=_bytes, sign=True)\n",
    "predictions.add_url_to_masks(signer) \\\n",
    "         .add_url_to_data(signer) \\\n",
    "         .assign_feature_schema_ids(OntologyBuilder.from_project(project)) \\\n",
    "         .add_to_dataset(dataset, client.upload_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-seafood",
   "metadata": {
    "id": "perfect-seafood"
   },
   "source": [
    "## **Optional** - Create labels with [Model Assisted Labeling](https://docs.labelbox.com/en/core-concepts/model-assisted-labeling)\n",
    "\n",
    "* Pre-label image so that we can quickly create ground truth\n",
    "* Create ground truth data for Model Diagnostics\n",
    "* Click on link below to label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "subject-painting",
   "metadata": {
    "id": "subject-painting"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "RUN_MAL = True\n",
    "if RUN_MAL:\n",
    "    project.enable_model_assisted_labeling()\n",
    "    # Convert from annotation types to import format\n",
    "    ndjson_predictions = NDJsonConverter.serialize(predictions)\n",
    "    upload_task = MALPredictionImport.create_from_objects(client, project.uid, f'mal-import-{uuid.uuid4()}',ndjson_predictions )\n",
    "    upload_task.wait_until_done()\n",
    "    print(upload_task.state , '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "MV4U1W4H_eMq",
   "metadata": {
    "id": "MV4U1W4H_eMq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.labelbox.com/go-label/cktimhae82yuq0y6dbralbxkq\n"
     ]
    }
   ],
   "source": [
    "print(f\"https://app.labelbox.com/go-label/{project.uid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-mandate",
   "metadata": {
    "id": "stopped-mandate"
   },
   "source": [
    "## Export Labels\n",
    "\n",
    "We do not support `Skipped` labels and have a limit of **2000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "excited-seminar",
   "metadata": {
    "id": "excited-seminar"
   },
   "outputs": [],
   "source": [
    "MAX_LABELS = 2000\n",
    "labels = [l for idx, l in enumerate(project.label_generator()) if idx < MAX_LABELS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-catering",
   "metadata": {
    "id": "smoking-catering"
   },
   "source": [
    "## Setup Model & Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "mental-minnesota",
   "metadata": {
    "id": "mental-minnesota"
   },
   "outputs": [],
   "source": [
    "lb_model = client.create_model(name = MODEL_NAME, ontology_id = project.ontology().uid)\n",
    "lb_model_run = lb_model.create_model_run(MODEL_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cu8h6h0g-Fe2",
   "metadata": {
    "id": "cu8h6h0g-Fe2"
   },
   "source": [
    "Select label ids to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "static-coordinate",
   "metadata": {
    "id": "static-coordinate"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_model_run.upsert_labels([label.uid for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g_u1ak2n7qn5",
   "metadata": {
    "id": "g_u1ak2n7qn5"
   },
   "source": [
    "### Compute Metrics\n",
    "* First get pairs of labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "hungry-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = get_label_pairs(labels, predictions, filter_mismatch = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-netherlands",
   "metadata": {},
   "source": [
    "* Create helper functions for our metrics\n",
    "* All functions will accept ground truth and prediction annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "muslim-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import cascaded_union\n",
    "\n",
    "def nearby_cars_iou(ground_truths, predictions, area_threshold = 17000):\n",
    "    \"\"\"\n",
    "    Metric to track the iou score for cars that are nearby (determined by pixel size).\n",
    "    \n",
    "    This might be useful to investigate why the model poorly when vehicles are nearby.\n",
    "    Or this might just be a metric we care a lot about optimizing because our self driving car needs to \n",
    "     be aware of its immediate surroundings for safety reasons.\n",
    "    \"\"\"\n",
    "    ground_truths = [gt for gt in ground_truths if gt.name == 'car']\n",
    "    predictions   = [pred for pred in predictions if pred.name == 'car']\n",
    "    ground_truths = cascaded_union([gt.value.shapely for gt in ground_truths if gt.value.shapely.area > area_threshold])\n",
    "    predictions   = cascaded_union([pred.value.shapely for pred in predictions if pred.value.shapely.area > area_threshold])\n",
    "    union = ground_truths.union(predictions).area\n",
    "    # If there is no prediction or label then the score is undefined\n",
    "    if union == 0:\n",
    "        return []\n",
    "    return [ScalarMetric(\n",
    "            value = ground_truths.intersection(predictions).area / union,\n",
    "            metric_name = \"iou\",\n",
    "            feature_name = \"car\",\n",
    "            subclass_name = \"nearby\" # Doesn't necessarily need to be a subclass in the ontology\n",
    "        )] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-rugby",
   "metadata": {},
   "source": [
    "* Compute and sssign each metric to prediction label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "committed-fairy",
   "metadata": {
    "id": "committed-fairy"
   },
   "outputs": [],
   "source": [
    "for (ground_truth, prediction) in pairs.values():\n",
    "    metrics = []\n",
    "    metrics.extend(feature_miou_metric(ground_truth.annotations, prediction.annotations))\n",
    "    metrics.extend(feature_confusion_matrix_metric(ground_truth.annotations, prediction.annotations))    \n",
    "    metrics.extend(nearby_cars_iou(ground_truth.annotations, prediction.annotations))\n",
    "    prediction.annotations.extend(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-illinois",
   "metadata": {},
   "source": [
    "### Upload to Labelbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "anonymous-addition",
   "metadata": {
    "id": "anonymous-addition"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotationImportState.FINISHED\n"
     ]
    }
   ],
   "source": [
    "upload_task = lb_model_run.add_predictions(f'diagnostics-import-{uuid.uuid4()}', NDJsonConverter.serialize(predictions))\n",
    "upload_task.wait_until_done()\n",
    "print(upload_task.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uTjGOyIW-3op",
   "metadata": {
    "id": "uTjGOyIW-3op"
   },
   "source": [
    "### Open Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "zrll9K6Q9tGK",
   "metadata": {
    "id": "zrll9K6Q9tGK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.labelbox.com/models/9c4ca3f6-8fc0-0595-8841-9581434e9672/9c4ca3f7-8a53-089f-fcc0-f246531fb14c/AllDatarowsSlice/2b404c28-059e-4163-a5d0-d77e6f329ab5?view=carousel\n",
      "https://app.labelbox.com/models/9c4ca3f6-8fc0-0595-8841-9581434e9672/9c4ca3f7-8a53-089f-fcc0-f246531fb14c/AllDatarowsSlice/533847fd-a3c7-48e6-886b-f9458497f403?view=carousel\n",
      "https://app.labelbox.com/models/9c4ca3f6-8fc0-0595-8841-9581434e9672/9c4ca3f7-8a53-089f-fcc0-f246531fb14c/AllDatarowsSlice/9f4833a4-ea2c-4f24-9283-82a6ea758b2e?view=carousel\n",
      "https://app.labelbox.com/models/9c4ca3f6-8fc0-0595-8841-9581434e9672/9c4ca3f7-8a53-089f-fcc0-f246531fb14c/AllDatarowsSlice/c1932683-d7c2-41c6-a186-a72569a4ebb0?view=carousel\n",
      "https://app.labelbox.com/models/9c4ca3f6-8fc0-0595-8841-9581434e9672/9c4ca3f7-8a53-089f-fcc0-f246531fb14c/AllDatarowsSlice/e9ce5747-1921-49ed-b648-286a7858cfd9?view=carousel\n"
     ]
    }
   ],
   "source": [
    "for idx, annotation_group in enumerate(lb_model_run.annotation_groups()):\n",
    "    if idx == 5:\n",
    "        break\n",
    "    print(annotation_group.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "billion-provider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7599894332796007, 'car'], [(1, 0, 0, 0), 'car'], [0.7599894332796007, 'car']]\n",
      "[[0.999990163223662, 'car'], [0.8, 'person'], [0.0, 'truck'], [(6, 0, 0, 0), 'car'], [(4, 1, 0, 0), 'person'], [(0, 1, 0, 0), 'truck'], [0.9999943560730941, 'car']]\n",
      "[[0.0, 'truck'], [(0, 1, 0, 0), 'truck']]\n",
      "[[0.6900109281465653, 'car'], [0.9090909090909091, 'person'], [(3, 1, 0, 0), 'car'], [(10, 1, 0, 0), 'person'], [0.8905576670555548, 'car']]\n",
      "[[0.8707948102228716, 'car'], [(3, 0, 0, 0), 'car'], [0.8899172370104976, 'car']]\n"
     ]
    }
   ],
   "source": [
    "for p in predictions:\n",
    "    print([[x.value, x.feature_name] for x in p.annotations if not isinstance(x.value, Geometry)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dietary-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from labelbox.data.annotation_types import Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-silly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model Diagnostics Demo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
