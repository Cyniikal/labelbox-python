# type: ignore
from labelbox.data.annotation_types.metrics.scalar import CustomScalarMetric
from typing import Dict, List, Optional, Tuple, Union
from shapely.geometry import Polygon
from itertools import product
import numpy as np
from collections import defaultdict

from ..annotation_types import (Label, ObjectAnnotation,
                                ClassificationAnnotation, Mask, Geometry, Point,
                                Line, Checklist, Text, Radio)



def subclass_ious(ground_truth: Label, prediction: Label) -> Dict[str, Optional[float]]:
    """
    # This function effectively flattens all Label classes and computes the iou.
    # Text is ignored for this function.
    # So for Radio or Checkbox  if you have an animal detection model and the model predicts:
    # Polygon - cat
        Radio - orange
        Checklist - fluffy

    # This all gets grouped into one category cat:orange:fluffy
    # This has to match


    The most appropriate use case for this is if you have one radio subclasses that you prefer to treat as top level.
    Otherwise this function is a bit naive and if you want something to specifically suite
    your use case then create a new function based off this one.
    """
    prediction_annotations = _create_feature_lookup(prediction.annotations)
    ground_truth_annotations = _create_feature_lookup(ground_truth.annotations)
    feature_schemas = set(prediction_annotations.keys()).union(
        set(ground_truth_annotations.keys()))


    def _create_classification_feature_lookup(annotations: Union[List[ObjectAnnotation], List[ClassificationAnnotation]]):
        # Note that these annotations should all be of the same type..

        if not len(annotations) or isinstance(annotations[0], ClassificationAnnotation):
            return annotations

        grouped_annotations = defaultdict(list)
        for annotation in annotations:
            row = []
            classifications = [classification.value for classification in annotation.classifications if isinstance(classification.value, Radio)]
            classifications = [classification.answer.name or classification.answer.feature_schema_id for classification in classifications ]
            # TODO: create the lookup
            grouped_annotations[annotation.name or annotation.feature_schema_id].append(annotation)

        return grouped_annotations


    ious = []
    for key in feature_schemas:
        # We shouldn't have any nones. Since the keys are generated by the presence of the object.
        prediction_annotations = prediction_annotations[key]
        ground_truth_annotations =



def feature_miou(ground_truth : Label, prediction: Label) -> List[CustomScalarMetric]:
    return [
        CustomScalarMetric(metric_name = "iou", metric_value = value, feature_name = name)
        for name, value in get_iou_across_features(ground_truth.annotations, prediction.annotations)
        if value is not None
    ]


# TODO: What should we call this?
# We should be returning these objects..
def data_row_miou_v2(ground_truth: Label, prediction: Label, include_subclasses = True) -> List[CustomScalarMetric]:
    return CustomScalarMetric(
        metric_name = "iou",
        metric_value = data_row_miou(ground_truth=ground_truth, prediction=prediction, include_subclasses = include_subclasses)
    )


def data_row_miou(ground_truth: Label, prediction: Label, include_subclasses = True) -> Optional[float]:
    """
    Calculate iou for two labels corresponding to the same data row.

    Args:
        ground_truth : Label containing human annotations or annotations known to be correct
        prediction: Label representing model predictions
    Returns:
        float indicating the iou score for this data row.
        Returns None if there are no annotations in ground_truth or prediction Labels
    """
    feature_ious = get_iou_across_features(ground_truth.annotations,
                                   prediction.annotations, include_subclasses)
    return average_ious(feature_ious)


def average_ious(feature_ious : Dict[str, Optional[float]]) -> Optional[float]:
    ious = [iou for iou in feature_ious.values() if iou is not None]
    return None if not len(ious) else np.mean(ious)


def get_iou_across_features(
    ground_truths: List[Union[ObjectAnnotation, ClassificationAnnotation]],
    predictions: List[Union[ObjectAnnotation, ClassificationAnnotation]],
    include_subclasses = True
) -> Optional[float]:
    """
    Groups annotations by feature_schema_id or name (which is available), calculates iou score and returns the mean across all features.

    Args:
        ground_truth : Label containing human annotations or annotations known to be correct
        prediction: Label representing model predictions
    Returns:
        float indicating the iou score for all features represented in the annotations passed to this function.
        Returns None if there are no annotations in ground_truth or prediction annotations
    """
    prediction_annotations = _create_feature_lookup(predictions)
    ground_truth_annotations = _create_feature_lookup(ground_truths)
    feature_schemas = set(prediction_annotations.keys()).union(
        set(ground_truth_annotations.keys()))
    ious = {
        feature_schema: feature_miou(ground_truth_annotations[feature_schema],
                     prediction_annotations[feature_schema], include_subclasses)
        for feature_schema in feature_schemas
    }
    return ious
    #ious = [iou for iou in ious if iou is not None] # TODO: What causes this to be None?

    return #None if not len(ious) else np.mean(ious)


def feature_miou(
    ground_truths: List[Union[ObjectAnnotation, ClassificationAnnotation]],
    predictions: List[Union[ObjectAnnotation, ClassificationAnnotation]],
    include_subclasses: bool
) -> Optional[float]:
    """
    Computes iou score for all features with the same feature schema id.

    Args:
        ground_truths: List of ground truth annotations with the same feature schema.
        predictions: List of annotations with the same feature schema.
    Returns:
        float representing the iou score for the feature type if score can be computed otherwise None.
    """
    if len(ground_truths) and not len(predictions):
        # No existing predictions but existing ground truths means no matches.
        return 0.
    elif not len(ground_truths) and len(predictions):
        # No ground truth annotations but there are predictions means no matches
        return 0.
    elif not len(ground_truths) and not len(predictions):
        # Ignore examples that do not have any annotations or predictions
        # This could maybe be counted as correct but could also skew the stats..
        return # Undefined (neither wrong nor right. )
    elif isinstance(predictions[0].value, Mask):
        return mask_miou(ground_truths, predictions, include_subclasses)
    elif isinstance(predictions[0].value, Geometry):
        return vector_miou(ground_truths, predictions, include_subclasses)
    elif isinstance(predictions[0], ClassificationAnnotation):
        return classification_miou(ground_truths, predictions)
    else:
        raise ValueError(
            f"Unexpected annotation found. Found {type(predictions[0].value)}")


def vector_miou(ground_truths: List[ObjectAnnotation],
                predictions: List[ObjectAnnotation],
                buffer=70., include_subclasses = True) -> float:
    """
    Computes iou score for all features with the same feature schema id.
    Calculation includes subclassifications.

    Args:
        ground_truths: List of ground truth vector annotations
        predictions: List of prediction vector annotations
    Returns:
        float representing the iou score for the feature type
    """
    pairs = _get_vector_pairs(ground_truths, predictions, buffer=buffer)
    pairs.sort(key=lambda triplet: triplet[2], reverse=True)
    solution_agreements = []
    solution_features = set()
    all_features = set()
    for prediction, ground_truth, agreement in pairs:
        all_features.update({id(prediction), id(ground_truth)})
        if id(prediction) not in solution_features and id(
                ground_truth) not in solution_features:
            solution_features.update({id(prediction), id(ground_truth)})
            if include_subclasses:
                classification_iou = average_ious(get_iou_across_features(
                    prediction.classifications, ground_truth.classifications))
                classification_iou = classification_iou if classification_iou is not None else agreement
                solution_agreements.append((agreement + classification_iou) / 2.)
            else:
                solution_agreements.append(agreement)

    # Add zeros for unmatched Features
    solution_agreements.extend([0.0] *
                               (len(all_features) - len(solution_features)))
    return np.mean(solution_agreements)


def mask_miou(ground_truths: List[ObjectAnnotation],
              predictions: List[ObjectAnnotation], include_subclasses = True) -> float:
    """
    Computes iou score for all features with the same feature schema id.
    Calculation includes subclassifications.

    Args:
        ground_truths: List of ground truth mask annotations
        predictions: List of prediction mask annotations
    Returns:
        float representing the iou score for the masks
    """
    prediction_np = np.max([pred.value.draw(color=1) for pred in predictions],
                           axis=0)
    ground_truth_np = np.max(
        [ground_truth.value.draw(color=1) for ground_truth in ground_truths],
        axis=0)
    if prediction_np.shape != ground_truth_np.shape:
        raise ValueError(
            "Prediction and mask must have the same shape."
            f" Found {prediction_np.shape}/{ground_truth_np.shape}.")

    agreement = _mask_iou(ground_truth_np, prediction_np)
    if not include_subclasses:
        return agreement

    prediction_classifications = []
    for prediction in predictions:
        prediction_classifications.extend(prediction.classifications)
    ground_truth_classifications = []
    for ground_truth in ground_truths:
        ground_truth_classifications.extend(ground_truth.classifications)

    classification_iou = get_iou_across_features(ground_truth_classifications,
                                                 prediction_classifications)

    classification_iou = classification_iou if classification_iou is not None else agreement
    return (agreement + classification_iou) / 2.


def classification_miou(ground_truths: List[ClassificationAnnotation],
                        predictions: List[ClassificationAnnotation]) -> float:
    """
    Computes iou score for all features with the same feature schema id.

    Args:
        ground_truths: List of ground truth classification annotations
        predictions: List of prediction classification annotations
    Returns:
        float representing the iou score for the classification
    """

    if len(predictions) != len(ground_truths) != 1:
        return 0.

    prediction, ground_truth = predictions[0], ground_truths[0]

    if type(prediction) != type(ground_truth):
        raise TypeError(
            "Classification features must be the same type to compute agreement. "
            f"Found `{type(prediction)}` and `{type(ground_truth)}`")

    if isinstance(prediction.value, Text):
        return text_iou(ground_truth.value, prediction.value)
    elif isinstance(prediction.value, Radio):
        return radio_iou(ground_truth.value, prediction.value)
    elif isinstance(prediction.value, Checklist):
        return checklist_iou(ground_truth.value, prediction.value)
    else:
        raise ValueError(f"Unexpected subclass. {prediction}")


def radio_iou(ground_truth: Radio, prediction: Radio) -> float:
    """
    Calculates agreement between ground truth and predicted radio values
    """
    return float(prediction.answer.feature_schema_id ==
                 ground_truth.answer.feature_schema_id)


def text_iou(ground_truth: Text, prediction: Text) -> float:
    """
    Calculates agreement between ground truth and predicted text
    """
    return float(prediction.answer == ground_truth.answer)


def checklist_iou(ground_truth: Checklist, prediction: Checklist) -> float:
    """
    Calculates agreement between ground truth and predicted checklist items
    """
    schema_ids_pred = {answer.feature_schema_id for answer in prediction.answer}
    schema_ids_label = {
        answer.feature_schema_id for answer in ground_truth.answer
    }
    return float(
        len(schema_ids_label & schema_ids_pred) /
        len(schema_ids_label | schema_ids_pred))


def _create_feature_lookup(
    annotations: List[Union[ObjectAnnotation, ClassificationAnnotation]]
) -> Dict[str, List[Union[ObjectAnnotation, ClassificationAnnotation]]]:
    """
    Groups annotation by schema id (if available otherwise name).

    Args:
        annotations: List of annotations to group
    Returns:
        a dict where each key is the feature_schema_id (or name)
        and the value is a list of annotations that have that feature_schema_id (or name)

    """
    # TODO: Add a check here.
    """

    We don't want to select name for one and then feature_schema_id for the other.
    I think in another function we should check

    Do we want to require that the user provides the feature name?
    We don't really want schema ids showing up in the metric names..

    So:

    Also add a freakin test.
    ####
    all_schema_ids_defined_pred, all_names_defined_pred = check_references(pred_annotations)
    if (not all_schema_ids_defined and not all_names_defined_pred):
        raise ValueError("All data must have feature_schema_ids or names set")


    all_schema_ids_defined_gt, all_names_defined_gt = check_references(gt_annotations)

    #Prefer name becuse the user will be able to know what it means
    #Schema id incase that doesn't exist..
    if (all_names_defined_pred and all_names_defined_gt):
        return 'name'
    elif all_schema_ids_defined_pred and all_schema_ids_defined_gt:
        return 'feature_schema_id'
    else:
        raise ValueError("Ground truth and prediction annotations must have set all name or feature ids. Otherwise there is no key to match on. Please update.")
    """
    grouped_annotations = defaultdict(list)
    for annotation in annotations:
        grouped_annotations[annotation.name or
                            annotation.feature_schema_id].append(annotation)

    return grouped_annotations


def _get_vector_pairs(
        ground_truths: List[ObjectAnnotation],
        predictions: List[ObjectAnnotation], buffer: float
) -> List[Tuple[ObjectAnnotation, ObjectAnnotation, float]]:
    """
    # Get iou score for all pairs of ground truths and predictions
    """
    pairs = []
    for prediction, ground_truth in product(predictions, ground_truths):
        if isinstance(prediction.value, Geometry) and isinstance(
                ground_truth.value, Geometry):
            if isinstance(prediction.value, (Line, Point)):
                score = _polygon_iou(prediction.value.shapely.buffer(buffer),
                                     ground_truth.value.shapely.buffer(buffer))
            else:
                score = _polygon_iou(prediction.value.shapely,
                                     ground_truth.value.shapely)
            pairs.append((prediction, ground_truth, score))
    return pairs


def _polygon_iou(poly1: Polygon, poly2: Polygon) -> float:
    """Computes iou between two shapely polygons."""
    if poly1.intersects(poly2):
        return poly1.intersection(poly2).area / poly1.union(poly2).area
    return 0.


def _mask_iou(mask1: np.ndarray, mask2: np.ndarray) -> float:
    """Computes iou between two binary segmentation masks."""
    return np.sum(mask1 & mask2) / np.sum(mask1 | mask2)
